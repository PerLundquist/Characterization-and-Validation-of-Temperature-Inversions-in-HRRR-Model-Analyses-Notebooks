{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba1f721",
   "metadata": {},
   "source": [
    "# Logger for Determining the Correctness in the High Resolution Rapid Refresh (HRRR) Modelled Representation of Temperature Inversions\n",
    "\n",
    "### Developed by Per Lundquist, M.S. Graduate Student, South Dakota Mines Atmospheric and Environmental Sciences\n",
    "\n",
    "Building off the work from Vector_Method.ipynb and Inversion_Comparison.ipynb, this notebook will collect 6 years of observed and modelled upper air sounding data from the National Weather Service (NWS) offices located in Rapid City, SD (KUNR), Riverton, WY (KRIW), Glasgow, MT (KGGW), and Denver, CO (KDNR). The program will then count the number of times the HRRR correctly produced the a temperature inversion, the number of times the HRRR correctly procuded an upper air sounding with no temperature inversion, the number of times the HRRR produced false positives, and the number of times the HRRR produced false negatives. This will be done for each individual station as well as the entire dataset. Graphical contingency tables for this data will be produced in a separate program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2fd04",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The imports here should be no different than those found in Inversion_Comparison.ipynb. Please follow the instructions in that notebook if you are having trouble finding the libraries you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from herbie import Herbie                               # main Herbie Module\n",
    "from toolbox.gridded_data import pluck_points           # finds the nearest lat/lon point. Comes from Brian Blaylock's Carpenter Workshop project\n",
    "from synoptic.services import stations_metadata         # Connects to Synoptic API. Comes from Blaylock's SynopticPy project\n",
    "\n",
    "import os                                               # A library for managing files and directories. Comes from the Standard Python Libraries\n",
    "import csv                                              # Self explanatory. A library for handling csv files\n",
    "\n",
    "import math                                                  # A math library from the Standar Python Libraries\n",
    "import numpy as np                                           # Numerical Python is a common number handling tool used in python\n",
    "import pandas as pd                                          # A popular data analytics tool in python developed by Wes McKinney. Here it is used to handle array objects such as upper air soundings\n",
    "pd.set_option('display.min_rows', 30)                        # Changing the default number of rows in a dataframe that pandas will display\n",
    "\n",
    "from datetime import datetime                                # A module from the standard python library for handling date objects. Will be used to import data at specific times, but is also handy in iterating through dates\n",
    "from datetime import timedelta                               # A datetime module needed to advance a datetime object\n",
    "from siphon.simplewebservice.wyoming import WyomingUpperAir  # Siphon is a tool developed by Unidata for querying data from various THREDDS servers. This particular module accesses data from the THREDDS server owned by the University of Wyoming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f896d83",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "The High Resolution Rapid Refresh (HRRR) Numerical Weather Prediction (NWP) model has been through many iterations. HRRRv1 was operational since 2014-09-30, HRRRv2 since 2016-08-23, HRRRv3 since 2018-07-12, and HRRRv4 since 2020-12-02. Through trial and error, it was found that of the 4 stations of interest, KUNR, KRIW, KGGW, and KDNR, HRRRv2, 3, and 4 had the ullest datasets. With that, and the fact that we want all days of the year to hold equal weight when using this dataset for climatological purposes, the time domain is from 2017-01-01 at 12 UTC through 2023-01-01 at 12 UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ecad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station identifier names formatted such that the Herbie or Siphon libraries can properly read them\n",
    "stids_HRRR    = ['KUNR', 'KRIW', 'KGGW', 'KDNR']\n",
    "stids_OBS     = ['RAP',  'RIW',  'GGW',  'DNR']\n",
    "\n",
    "# Time domain\n",
    "start_time    = datetime.strptime('2017-01-01_12', \"%Y-%m-%d_%H\")\n",
    "stop_time     = datetime.strptime('2023-01-01_12', \"%Y-%m-%d_%H\")\n",
    "\n",
    "# local directory holding sounding data\n",
    "sounding_dir  = './sounding_csvs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb043e",
   "metadata": {},
   "source": [
    "## Ingesting upper air data\n",
    "\n",
    "This portion of the code will produce two pandas dataframes for the observed upper air data and the modelled upper air data. Two options for doing so are presented: downloading from the cloud through Herbie (for modelled data) and siphon (for observed data), or referencing already downloaded data from a local source, the latter of which is significantly faster. A local database of upperair profiles is provided with this notebook for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1154e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "### GRABBING THE HRRR DATA VIA HERBIE ###\n",
    "#########################################\n",
    "\n",
    "# A revised key_err_fix() function that should grab the index for the xarray dataframe we want from Herbie, regardless\n",
    "# of the coordinate system\n",
    "def key_err_fix(dataframe, key_, orog_check=0):\n",
    "    \n",
    "    for i in range(len(dataframe)):\n",
    "        \n",
    "        coords_keys    = list(dataframe[i].coords.keys())\n",
    "        data_vars_keys = list(dataframe[i].data_vars.keys())\n",
    "        \n",
    "        if orog_check == 1:\n",
    "            if any(ele in data_vars_keys for ele in [key_]):\n",
    "                return i\n",
    "        else:\n",
    "            if any(ele in coords_keys for ele in ['isobaricInhPa']) and any(ele in data_vars_keys for ele in [key_]):\n",
    "                return i\n",
    "            \n",
    "            \n",
    "# A function for grabbing Herbie data depending on the variable we want\n",
    "# run_time = the date of interest\n",
    "# var      = the requested data variable, in this case temperature or orography\n",
    "# stid     = the Herbie compatible station identifier. Typically, it's different than the one used in siphon\n",
    "# model_   = the NWP model to be used\n",
    "# product_ = the type of product coordinates to be used\n",
    "def get_HRRR_data(run_time, var, stid, model_='HRRR', product_='prs'):\n",
    "   \n",
    "    print('\\nGrabbing HRRR '+var+' data for '+run_time)\n",
    "    \n",
    "    # A lookup table with the appropriate data variable key from the xarray dataframe needed\n",
    "    match var:\n",
    "        case \"TMP\":\n",
    "            key_ = 't'\n",
    "        case \"DPT\":\n",
    "            key_ = 'dpt'\n",
    "        case \"U\":\n",
    "            key_ = 'u'\n",
    "        case \"V\":\n",
    "            key_ = 'v'\n",
    "        case \"W\":\n",
    "            var = \"V\"\n",
    "            key_ = 'w'\n",
    "        case \"RH\":\n",
    "            key_ = 'r'\n",
    "        case \"HGT\":\n",
    "            key_ = 'gh'\n",
    "        case \"OROG\":\n",
    "            var = \"HGT\"\n",
    "            key_ = 'orog'\n",
    "    \n",
    "    # Get HRRR dataframe\n",
    "    try:\n",
    "        # make initial Herbie object\n",
    "        H_xf = Herbie(run_time, model=model_, product=product_, verbose=False).xarray(var)\n",
    "        \n",
    "        # in order to use key_err_fix, the Herbie object must be a list of xarray dataframes. If there's only one\n",
    "        # dataframe, Herbie won't put it in a list, so we'll fix that here\n",
    "        if isinstance(H_xf, list):\n",
    "            pass\n",
    "        else:\n",
    "            H_xf =[H_xf]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('time ' + run_time + ' could not be collected. Omitting from records............\\n')\n",
    "        \n",
    "        if key_ == 'orog':\n",
    "            return 'nan'\n",
    "        else:\n",
    "            return 'nan','nan','nan'\n",
    "    \n",
    "    # set lookup index\n",
    "    if key_ == 'orog':\n",
    "        index_ = key_err_fix(H_xf, key_, orog_check=1)\n",
    "    else:\n",
    "        index_ = key_err_fix(H_xf, key_)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Get Point Data\n",
    "        closest_staions_array = stations_metadata(radius=stid+\",0\",verbose=False) \n",
    "        station_points = np.array(list(zip(closest_staions_array.loc[\"longitude\"], closest_staions_array.loc[\"latitude\"])))\n",
    "        station_names = closest_staions_array.loc[\"STID\"].to_numpy()\n",
    "\n",
    "        # make secondary xarray object with vertical profile of the variable we want a the location we want\n",
    "        ds_pluck = pluck_points(H_xf[index_], station_points[:1], station_names[:1],verbose=False) # see lookup table in above cell\n",
    "        \n",
    "        # confirm we have the coordinate system we want\n",
    "        if any(coord_name == 'isobaricInhPa' for coord_name in ds_pluck.coords):\n",
    "        \n",
    "            # number of levels to work with\n",
    "            levels = len(ds_pluck.coords['isobaricInhPa'])\n",
    "\n",
    "            # save data to lists. IT'S IMPORTANT THAT THEY'RE LISTS NOT NUMPY ARRAYS YET!!!\n",
    "            var_out = []\n",
    "            pres = []\n",
    "\n",
    "            for i in range(levels):\n",
    "                var_out.append(float(ds_pluck.data_vars[key_][0,i])) # see lookup table above\n",
    "                pres.append(float(ds_pluck.coords['isobaricInhPa'][i]))\n",
    "\n",
    "\n",
    "            # return the variables\n",
    "            return var_out, pres, levels\n",
    "        \n",
    "        # just return one variable if different coords\n",
    "        else:\n",
    "            \n",
    "            return float(ds_pluck.data_vars[key_])\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print('time ' + run_time + ' could not be collected. Omitting from records............\\n')\n",
    "        \n",
    "        if key_ == 'orog':\n",
    "            return 'nan'\n",
    "        else:\n",
    "            return 'nan','nan','nan'\n",
    "    \n",
    "\n",
    "# Grab a raw modelled profile\n",
    "def mk_HRRR_df(curr_date, stid, directory):\n",
    "    \n",
    "    print('\\nGrabbing HRRR data for '+curr_date)\n",
    "    \n",
    "    # Check if a file for the date exists\n",
    "    local_available = False\n",
    "    \n",
    "    input_date      = datetime.strptime(curr_date, \"%Y-%m-%d %H:%M\")\n",
    "    file_name       = input_date.strftime(\"%Y-%m-%d_%HUTC.csv\")\n",
    "    \n",
    "    file_loc        = directory + stid + '/HRRR/'\n",
    "    \n",
    "    file_path       = os.path.join(file_loc, file_name)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        local_available = True\n",
    "\n",
    "    # Download the profile if no local file available\n",
    "    if not local_available:\n",
    "        \n",
    "        # Turn this on if you don't want to try to download data (downloading data is pretty slow...)\n",
    "#         print(\"no HRRR data available\")\n",
    "#         return 'nan'        \n",
    "\n",
    "        # get HRRR data\n",
    "        height_HRRR, pres_HRRR, levels_HRRR = get_HRRR_data(curr_date, 'HGT', stid)\n",
    "        temp_HRRR, pres_HRRR, levels_HRRR = get_HRRR_data(curr_date, 'TMP', stid)\n",
    "        dwpt_HRRR, pres_HRRR, levels_HRRR = get_HRRR_data(curr_date, 'DPT', stid)\n",
    "        u_HRRR, pres_HRRR, levels_HRRR = get_HRRR_data(curr_date, 'U', stid)\n",
    "        v_HRRR, pres_HRRR, levels_HRRR = get_HRRR_data(curr_date, 'V', stid)\n",
    "\n",
    "        # surface elev (labeled Geopotential Height - Orography in the file)\n",
    "        sfc_elev = get_HRRR_data(curr_date, 'OROG', stid)\n",
    "\n",
    "        # Check if data collection was successful:\n",
    "        list_check = [sfc_elev, height_HRRR, temp_HRRR, dwpt_HRRR, u_HRRR, v_HRRR, pres_HRRR, levels_HRRR, sfc_elev]\n",
    "        bool_check = False\n",
    "        \n",
    "        for ele in list_check:\n",
    "            if type(ele) == str:\n",
    "                if ele == 'nan':\n",
    "                    bool_check = True\n",
    "                    break\n",
    "                    \n",
    "        if bool_check:\n",
    "            return 'nan'\n",
    "        \n",
    "        # adjust temps because they're in K\n",
    "        # adjust height because it's in units of meters above msl\n",
    "        temp_HRRR = [temp - 273.15 for temp in temp_HRRR]\n",
    "        dwpt_HRRR = [dwpt - 273.15 for dwpt in dwpt_HRRR]\n",
    "\n",
    "        height_HRRR = [height - sfc_elev for height in height_HRRR]\n",
    "\n",
    "\n",
    "        # put it all in a dataframe\n",
    "        HRRR_df = pd.DataFrame({\n",
    "            'Pressure level (hPa)': pres_HRRR,\n",
    "            'Height AGL (m)': height_HRRR,\n",
    "            'Temp (°C)': temp_HRRR,\n",
    "            'Dwpt (°C)': dwpt_HRRR,\n",
    "            'U wind (m/s)': u_HRRR,\n",
    "            'V wind (m/s)': v_HRRR\n",
    "        })\n",
    "        \n",
    "        # save the dataframe to file\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        if not os.path.exists(directory + stid):\n",
    "            os.mkdir(directory + stid)\n",
    "        if not os.path.exists(file_loc):\n",
    "            os.mkdir(file_loc)\n",
    "            \n",
    "        HRRR_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        return HRRR_df\n",
    "\n",
    "    \n",
    "    # Grab the file if it's found\n",
    "    else:\n",
    "        HRRR_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "        \n",
    "        # Sometimes the temp and dwpt columns are labelled poorly. This will fix that\n",
    "        if 'Temp (°C)' not in HRRR_df.columns:\n",
    "            alt_label = 'Temp (Â°C)'\n",
    "            if alt_label in HRRR_df.columns:\n",
    "                HRRR_df.rename(columns={alt_label: 'Temp (°C)'}, inplace=True)\n",
    "                \n",
    "        if 'Dwpt (°C)' not in HRRR_df.columns:\n",
    "            alt_label = 'Dwpt (Â°C)'\n",
    "            if alt_label in HRRR_df.columns:\n",
    "                HRRR_df.rename(columns={alt_label: 'Dwpt (°C)'}, inplace=True)\n",
    "        \n",
    "        # Use the first height in this entry to adjust to height AGL\n",
    "        sfc_elev = float(HRRR_df['Height AGL (m)'].iloc[0])\n",
    "        HRRR_df['Height AGL (m)'] = [h - sfc_elev for h in HRRR_df['Height AGL (m)']]\n",
    "        \n",
    "        return HRRR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349402dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### GRABBING THE OBSERVED DATA VIA SIPHON ###\n",
    "#############################################\n",
    "def get_OBS_data(run_time, stid, obs_stid, directory):\n",
    "    \n",
    "    print('\\nGrabbing OBS data for '+run_time)\n",
    "    \n",
    "    # Check if a file for the date exists\n",
    "    local_available = False\n",
    "    \n",
    "    input_date      = datetime.strptime(run_time, \"%Y-%m-%d %H:%M\")\n",
    "    file_name       = input_date.strftime(\"%Y-%m-%d_%HUTC.csv\")\n",
    "    \n",
    "    file_loc        = directory + stid + '/OBS/'\n",
    "    \n",
    "    file_path       = os.path.join(file_loc, file_name)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        local_available = True\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if not local_available:\n",
    "            #Provide date/time: year, month, day, hour\n",
    "            date = datetime.strptime(run_time, \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "            #And download the data\n",
    "            df = WyomingUpperAir.request_data(date, obs_stid)\n",
    "\n",
    "            # adjust to be in meters AGL\n",
    "            sfc_elev = df['height'].iloc[0]\n",
    "            df['height'] = [h - sfc_elev for h in df['height']]\n",
    "\n",
    "            #Define some variables to make things easy\n",
    "            p= df['pressure'].values\n",
    "            h= df['height'].values\n",
    "            t= df['temperature'].values\n",
    "            td= df['dewpoint'].values\n",
    "            u= df['u_wind'].values\n",
    "            v= df['v_wind'].values\n",
    "\n",
    "            better_df = pd.DataFrame({\n",
    "                'Pressure level (hPa)': p,\n",
    "                'Height AGL (m)': h,\n",
    "                'Temp (°C)': t,\n",
    "                'Dwpt (°C)': td,\n",
    "                'U wind (m/s)': u,\n",
    "                'V wind (m/s)': v\n",
    "            })\n",
    "            \n",
    "            # save the dataframe to file\n",
    "            if not os.path.exists(directory):\n",
    "                os.mkdir(directory)\n",
    "            if not os.path.exists(directory + stid):\n",
    "                os.mkdir(directory + stid)\n",
    "            if not os.path.exists(file_loc):\n",
    "                os.mkdir(file_loc)\n",
    "\n",
    "            better_df.to_csv(file_path, index=False)\n",
    "\n",
    "            return better_df\n",
    "        \n",
    "        # Grab the file if it's found\n",
    "        else:\n",
    "            OBS_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "            \n",
    "            # Sometimes the temp and dwpt columns are labelled poorly. This will fix that\n",
    "            if 'Temp (°C)' not in OBS_df.columns:\n",
    "                alt_label = 'Temp (Â°C)'\n",
    "                if alt_label in OBS_df.columns:\n",
    "                    OBS_df.rename(columns={alt_label: 'Temp (°C)'}, inplace=True)\n",
    "                    \n",
    "            if 'Dwpt (°C)' not in OBS_df.columns:\n",
    "                alt_label = 'Dwpt (Â°C)'\n",
    "                if alt_label in OBS_df.columns:\n",
    "                    OBS_df.rename(columns={alt_label: 'Dwpt (°C)'}, inplace=True)\n",
    "            \n",
    "            sfc_elev = OBS_df['Height AGL (m)'].iloc[0]\n",
    "            OBS_df['Height AGL (m)'] = [h - sfc_elev for h in OBS_df['Height AGL (m)']]\n",
    "            \n",
    "            return OBS_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 'nan'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316f54d",
   "metadata": {},
   "source": [
    "## Functions to obtain primary inversions\n",
    "\n",
    "For a given upper air sounding, we will need tp filter out the primary temperature inversion of interest, if any. This portion follows the procedure from Inversion_Comparison.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### SLICE THE DATAFRAMES TO START AT THE SAME HEIGHT ###\n",
    "########################################################\n",
    "def start_at_same_height(OBS_df, HRRR_df):\n",
    "    \n",
    "    new_OBS_df  = OBS_df\n",
    "    new_HRRR_df = HRRR_df\n",
    "    \n",
    "    first_obs_height  = new_OBS_df['Height AGL (m)'].iloc[0]\n",
    "    first_hrrr_height = new_HRRR_df['Height AGL (m)'].iloc[0]\n",
    "\n",
    "    # if the sfc value on the observed data is below the sfc on the hrrr, cut it off\n",
    "    if first_obs_height < first_hrrr_height:\n",
    "        obs_sfc_data = pd.DataFrame({\n",
    "            'Pressure level (hPa)': [\n",
    "                linear_fix(first_hrrr_height, new_OBS_df['Height AGL (m)'], new_OBS_df['Pressure level (hPa)'], increase_with_height=True)],\n",
    "            'Height AGL (m)': [first_hrrr_height],\n",
    "            'Temp (°C)': [\n",
    "                linear_fix(first_hrrr_height, new_OBS_df['Height AGL (m)'], new_OBS_df['Temp (°C)'], increase_with_height=True)],\n",
    "            'Dwpt (°C)': [\n",
    "                linear_fix(first_hrrr_height, new_OBS_df['Height AGL (m)'], new_OBS_df['Dwpt (°C)'], increase_with_height=True)]\n",
    "        })\n",
    "\n",
    "        # remove values below the first height in modelled data\n",
    "        new_OBS_df = new_OBS_df[new_OBS_df['Height AGL (m)'] >= first_hrrr_height]\n",
    "\n",
    "        # add the new sfc data to it and sort\n",
    "        new_OBS_df = pd.concat([obs_sfc_data, new_OBS_df])\n",
    "        new_OBS_df = OBS_df.sort_values(by='Height AGL (m)', ascending=True)\n",
    "        new_OBS_df = OBS_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # if the sfc value on the hrrr is below the sfc on the observed data, cut it off\n",
    "    elif first_obs_height > first_hrrr_height:\n",
    "        hrrr_sfc_data = pd.DataFrame({\n",
    "            'Pressure level (hPa)': [\n",
    "                linear_fix(first_obs_height, new_HRRR_df['Height AGL (m)'], new_HRRR_df['Pressure level (hPa)'],\n",
    "                           increase_with_height=True)],\n",
    "            'Height AGL (m)': [first_obs_height],\n",
    "            'Temp (°C)': [linear_fix(first_obs_height, new_HRRR_df['Height AGL (m)'], new_HRRR_df['Temp (°C)'],\n",
    "                                     increase_with_height=True)],\n",
    "            'Dwpt (°C)': [linear_fix(first_obs_height, new_HRRR_df['Height AGL (m)'], new_HRRR_df['Dwpt (°C)'],\n",
    "                                     increase_with_height=True)]\n",
    "        })\n",
    "\n",
    "        # remove values below the first height in observed data\n",
    "        new_HRRR_df = HRRR_df[new_HRRR_df['Height AGL (m)'] >= first_obs_height]\n",
    "\n",
    "        # add the new sfc data to it and sort\n",
    "        new_HRRR_df = pd.concat([hrrr_sfc_data, new_HRRR_df])\n",
    "        HRRR_df = HRRR_df.sort_values(by='Height AGL (m)', ascending=True)\n",
    "        HRRR_df = HRRR_df.reset_index(drop=True)\n",
    "        \n",
    "    return new_OBS_df, new_HRRR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "### THE LINEAR INTERPOLATION FUNCTION USED EVERYWHERE ###\n",
    "#########################################################\n",
    "\n",
    "# conduct a linear interpolation between points.\n",
    "\n",
    "# ref_coord            = the desired location you want to conduct a linear interpolation. (y in the above equation)\n",
    "# unfixed_data_coords  = array of the coordinate values in the raw data (contains y_above and y_below)\n",
    "# unfixed_data_values  = array of the variable values from the raw data (contains x_above and x_below)\n",
    "# increase_with_height = a boolean to tell the function if your height coordinate is increasing or decreasing with\n",
    "#                        height. Used to switch between height and pressure coordinates\n",
    "def linear_fix(ref_coord, unfixed_data_coords, unfixed_data_values, increase_with_height=False):\n",
    "    \n",
    "    # Rudimentary error checking. Doesn't stop the program, but won't create an interpolated dataframe\n",
    "    if len(unfixed_data_coords) != len(unfixed_data_values):\n",
    "        print('The variable and pressure arrays must be of equal length. Cancelling interpolation......')\n",
    "        return\n",
    "    \n",
    "    # Height coordinates (function is the same in both scenarios)\n",
    "    if increase_with_height:\n",
    "        \n",
    "        # scan the data until we find the first point above our reference point\n",
    "        for i in range(len(unfixed_data_values)):\n",
    "            if unfixed_data_coords[i] > ref_coord:\n",
    "                # interpolate temp and rh between pressure levels above/below surface pressure to get surface values\n",
    "                #\n",
    "                # Equation: x = x_above + ( ((x_below - x_above) * (height_above - ref_height)) / (height_above - height_below) )\n",
    "\n",
    "                fixed_value = unfixed_data_values[i] + (((unfixed_data_values[i - 1] - unfixed_data_values[i]) * (\n",
    "                            unfixed_data_coords[i] - ref_coord)) / (unfixed_data_coords[i] - unfixed_data_coords[\n",
    "                    i - 1]))\n",
    "                return fixed_value\n",
    "                break  # This won't run, but I put it here just in case. Perhaps I have anxiety, haha\n",
    "    \n",
    "    # Pressure coordinates\n",
    "    else:\n",
    "        \n",
    "        # scan the data until we find the first point above our reference point\n",
    "        for i in range(len(unfixed_data_values)):\n",
    "            if unfixed_data_coords[i] < ref_coord:\n",
    "                # interpolate temp and rh between pressure levels above/below surface pressure to get surface values\n",
    "                #\n",
    "                # Equation: x = x_above + ( ((x_below - x_above) * (pressure_above - ref_pressure)) / (pressure_above - pressure_below) )\n",
    "\n",
    "                fixed_value = unfixed_data_values[i] + (((unfixed_data_values[i - 1] - unfixed_data_values[i]) * (\n",
    "                            unfixed_data_coords[i] - ref_coord)) / (unfixed_data_coords[i] - unfixed_data_coords[\n",
    "                    i - 1]))\n",
    "                return fixed_value\n",
    "                break  # just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### CONVERT THE DATAFRAME TO A STANDARD HEIGHT COORDINATE ###\n",
    "#############################################################\n",
    "\n",
    "# Note: instead of locking the cutoff height to 5km, we will allow the option to change the cutoff height. This is used\n",
    "# later to explore ways of improving the model correctness in creating inversions. Nevertheless, when running the normalization\n",
    "# function, the domain of [0, 5000] is still used. The cutoff is not utilized there\n",
    "\n",
    "def mk_height_df(OBS_df, HRRR_df, cutoff=5000.):\n",
    "    \n",
    "    # Make a new dataframe for the OBSERVED data such that datapoints are every 100m\n",
    "    standard_heights = np.arange(0, cutoff, cutoff/50.)\n",
    "\n",
    "    # Make arrays for each variable we care about and conduct linear interpolation    \n",
    "    pressures = []\n",
    "    temps     = []\n",
    "    dwpts     = []\n",
    "\n",
    "    for h in standard_heights:\n",
    "        pressures.append(linear_fix(h, OBS_df['Height AGL (m)'], OBS_df['Pressure level (hPa)'], increase_with_height=True))\n",
    "        temps.append(linear_fix(h, OBS_df['Height AGL (m)'], OBS_df['Temp (°C)'], increase_with_height=True))\n",
    "        dwpts.append(linear_fix(h, OBS_df['Height AGL (m)'], OBS_df['Dwpt (°C)'], increase_with_height=True))\n",
    "\n",
    "    OBS_df_standard = pd.DataFrame({\n",
    "        'Pressure level (hPa)': pressures,\n",
    "        'Height AGL (m)': standard_heights,\n",
    "        'Temp (°C)': temps,\n",
    "        'Dwpt (°C)': dwpts\n",
    "    })\n",
    "    \n",
    "    # set up all the lapse rates through finite differencing\n",
    "\n",
    "    # initialize a new lapse rate column that is full of zeros\n",
    "    OBS_df_standard['lapse_rate'] = 0\n",
    "\n",
    "    # loop through every point in the dataframe, assigning a lapse rate value\n",
    "    for i in range(len(OBS_df_standard['Temp (°C)'])):\n",
    "\n",
    "        if i == ( len(OBS_df_standard['Temp (°C)']) - 1 ): # fill the final point. just say it's the dry adiabatic lapse rate\n",
    "            OBS_df_standard['lapse_rate'].iloc[i] = -9.7670 * (1/1000.)  # °C/m\n",
    "        else:\n",
    "            OBS_df_standard['lapse_rate'].iloc[i] = (OBS_df_standard['Temp (°C)'].iloc[i+1] - OBS_df_standard['Temp (°C)'].iloc[i]) / (OBS_df_standard['Height AGL (m)'].iloc[i+1] - OBS_df_standard['Height AGL (m)'].iloc[i])\n",
    "    \n",
    "    # Do the same to the HRRR data\n",
    "    pressures = []\n",
    "    temps     = []\n",
    "    dwpts     = []\n",
    "\n",
    "    for h in standard_heights:\n",
    "        pressures.append(linear_fix(h, HRRR_df['Height AGL (m)'], HRRR_df['Pressure level (hPa)'], increase_with_height=True))\n",
    "        temps.append(linear_fix(h, HRRR_df['Height AGL (m)'], HRRR_df['Temp (°C)'], increase_with_height=True))\n",
    "        dwpts.append(linear_fix(h, HRRR_df['Height AGL (m)'], HRRR_df['Dwpt (°C)'], increase_with_height=True))\n",
    "\n",
    "    HRRR_df_standard = pd.DataFrame({\n",
    "        'Pressure level (hPa)': pressures,\n",
    "        'Height AGL (m)': standard_heights,\n",
    "        'Temp (°C)': temps,\n",
    "        'Dwpt (°C)': dwpts\n",
    "    })\n",
    "\n",
    "    # set up all the lapse rates through finite differencing\n",
    "\n",
    "    # initialize a new lapse rate column that is full of zeros\n",
    "    HRRR_df_standard['lapse_rate'] = 0\n",
    "\n",
    "    # loop through every point in the dataframe, assigning a lapse rate value\n",
    "    for i in range(len(HRRR_df_standard['Temp (°C)'])):\n",
    "\n",
    "        if i == ( len(HRRR_df_standard['Temp (°C)']) - 1 ): # fill the final point. just say it's the dry adiabatic lapse rate\n",
    "            HRRR_df_standard['lapse_rate'].iloc[i] = -9.7670 * (1/1000.)  # °C/m\n",
    "        else:\n",
    "            HRRR_df_standard['lapse_rate'].iloc[i] = (HRRR_df_standard['Temp (°C)'].iloc[i+1] - HRRR_df_standard['Temp (°C)'].iloc[i]) / (HRRR_df_standard['Height AGL (m)'].iloc[i+1] - HRRR_df_standard['Height AGL (m)'].iloc[i])\n",
    "            \n",
    "    \n",
    "    return OBS_df_standard, HRRR_df_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "### CONVERT THE DATAFRAME TO A NORMALIZED HEIGHT COORDINATE ###\n",
    "###############################################################\n",
    "\n",
    "def mk_normalized_height_df(OBS_df, HRRR_df):\n",
    "    # take the values in standard_heights and OBS_df_standard and normalize to the specified range\n",
    "    # Heights will be nomalized from a range of [0, 5000] to [0, 1]\n",
    "    # Temperatures will be normalized from a range of [-39, 40] to [0, 1]\n",
    "\n",
    "    normalized_heights = np.arange(0, 1.0, (100/5000.)) # (start value, end value, increment)\n",
    "\n",
    "    temps = np.zeros(shape=len(normalized_heights))\n",
    "    dwpts = np.zeros(shape=len(normalized_heights))\n",
    "\n",
    "    for i in range(len(normalized_heights)):\n",
    "\n",
    "        # adding 39 puts it to a range of [0, 79], then simply divide by 79 to get [0, 1]\n",
    "        if OBS_df_standard['Temp (°C)'][i] is not None:\n",
    "            temps[i] = (OBS_df_standard['Temp (°C)'][i] + 39.) / (39.+40.)\n",
    "        if OBS_df_standard['Dwpt (°C)'][i] is not None:\n",
    "            dwpts[i] = (OBS_df_standard['Dwpt (°C)'][i] + 39.) / (39.+40.)\n",
    "\n",
    "    OBS_df_normalized = pd.DataFrame({\n",
    "        'Pressure level (hPa)': OBS_df_standard['Pressure level (hPa)'][i],\n",
    "        'Height AGL (m)': normalized_heights,\n",
    "        'Temp (°C)': temps,\n",
    "        'Dwpt (°C)': dwpts\n",
    "    })\n",
    "\n",
    "    # set up all the lapse rates through finite differencing\n",
    "\n",
    "    # initialize a new lapse rate column that is full of zeros\n",
    "    OBS_df_normalized['lapse_rate'] = 0\n",
    "\n",
    "    # loop through every point in the dataframe, assigning a lapse rate value\n",
    "    for i in range(len(OBS_df_normalized['Temp (°C)'])):\n",
    "\n",
    "        if i == ( len(OBS_df_normalized['Temp (°C)']) - 1 ): # fill the final point. just say it's the dry adiabatic lapse rate\n",
    "            OBS_df_normalized['lapse_rate'].iloc[i] = -9.7670 * (1/1000.)  # °C/m\n",
    "        else:\n",
    "            OBS_df_normalized['lapse_rate'].iloc[i] = (OBS_df_normalized['Temp (°C)'].iloc[i+1] - OBS_df_normalized['Temp (°C)'].iloc[i]) / (OBS_df_normalized['Height AGL (m)'].iloc[i+1] - OBS_df_normalized['Height AGL (m)'].iloc[i])\n",
    "            \n",
    "    \n",
    "    # Likewise with HRRR data\n",
    "\n",
    "    temps = np.zeros(shape=len(normalized_heights))\n",
    "    dwpts = np.zeros(shape=len(normalized_heights))\n",
    "\n",
    "    for i in range(len(normalized_heights)):\n",
    "\n",
    "        # adding 39 puts it to a range of [0, 79], then simply divide by 79 to get [0, 1]\n",
    "        if HRRR_df_standard['Temp (°C)'][i] is not None:\n",
    "            temps[i] = (HRRR_df_standard['Temp (°C)'][i] + 39.) / (39.+40.)\n",
    "        if HRRR_df_standard['Dwpt (°C)'][i] is not None:\n",
    "            dwpts[i] = (HRRR_df_standard['Dwpt (°C)'][i] + 39.) / (39.+40.)\n",
    "\n",
    "    HRRR_df_normalized = pd.DataFrame({\n",
    "        'Pressure level (hPa)': HRRR_df_standard['Pressure level (hPa)'][i],\n",
    "        'Height AGL (m)': normalized_heights,\n",
    "        'Temp (°C)': temps,\n",
    "        'Dwpt (°C)': dwpts\n",
    "    })\n",
    "\n",
    "    # initialize a new lapse rate column that is full of zeros\n",
    "    HRRR_df_normalized['lapse_rate'] = 0\n",
    "\n",
    "    # loop through every point in the dataframe, assigning a lapse rate value\n",
    "    for i in range(len(HRRR_df_normalized['Temp (°C)'])):\n",
    "\n",
    "        if i == ( len(HRRR_df_normalized['Temp (°C)']) - 1 ): # fill the final point. just say it's the dry adiabatic lapse rate\n",
    "            HRRR_df_normalized['lapse_rate'].iloc[i] = -9.7670 * (1/1000.)  # °C/m\n",
    "        else:\n",
    "            HRRR_df_normalized['lapse_rate'].iloc[i] = (HRRR_df_normalized['Temp (°C)'].iloc[i+1] - HRRR_df_normalized['Temp (°C)'].iloc[i]) / (HRRR_df_normalized['Height AGL (m)'].iloc[i+1] - HRRR_df_normalized['Height AGL (m)'].iloc[i])\n",
    "            \n",
    "    \n",
    "    return OBS_df_normalized, HRRR_df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### GRAB THE PRIMARY TEMPERATURE INVERSIONS ###\n",
    "###############################################\n",
    "def inversion_grabber(OBS_df_standard, OBS_df_norm, HRRR_df_standard, HRRR_df_norm):\n",
    "    \n",
    "    # Grab the inversion lists in regular coordinates\n",
    "    \n",
    "    \n",
    "    # Follow Vector_Method.ipynb to create a plot with both observed and modelled inversion vectors on it\n",
    "    # We'll just use a list to save the inversions\n",
    "    OBS_inversions = []\n",
    "\n",
    "    # booleans to check if we started/finished grabbing an inversion\n",
    "    start_grabbed = 0\n",
    "    end_grabbed = 0\n",
    "\n",
    "    # Scan all points to find positive lapse rates\n",
    "    for i in range(len(OBS_df_standard['lapse_rate'])):\n",
    "\n",
    "        # Positive lapse rate found! Save this coordiate as [height, temp]\n",
    "        if ( OBS_df_standard['lapse_rate'][i] >= 0 ) and ( start_grabbed == 0):\n",
    "            start_coord = [OBS_df_standard['Height AGL (m)'][i], OBS_df_standard['Temp (°C)'][i]]\n",
    "            start_grabbed = 1 # we just grabbed the start point, so tell the program we did\n",
    "            end_grabbed = 0   # haven't grabbed the ending point though...\n",
    "\n",
    "        # The lapse rate above this point is no longer positive.... Guess we hit the end of the inversion\n",
    "        # Note, this part can only run if we've already started grabbing an inversion\n",
    "        elif ( OBS_df_standard['lapse_rate'][i] < 0 ) and ( start_grabbed == 1):\n",
    "            end_coord = [OBS_df_standard['Height AGL (m)'][i], OBS_df_standard['Temp (°C)'][i]]\n",
    "            start_grabbed = 0 # We finished grabbing this inversion, so we can tell the program it's ready to start a new one\n",
    "            end_grabbed = 1   # Finished grabbing this inversion, so let's say that we grabbed it\n",
    "\n",
    "        # save the coordinates, ensuring that we finished grabbing this inversion and the program is ready for the next one\n",
    "        if end_grabbed == 1 and start_grabbed == 0:\n",
    "            OBS_inversions.append([start_coord, end_coord])\n",
    "            start_grabbed = 0\n",
    "            end_grabbed = 0\n",
    "\n",
    "\n",
    "    # Calculate distances and store them with their respective line segments\n",
    "    data = [] # simple list to store the data of each inversion. Not organized quite yet\n",
    "\n",
    "    for line in OBS_inversions:\n",
    "        point1, point2 = line\n",
    "        distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2) # Distance formula\n",
    "\n",
    "        # Extract relevant information\n",
    "        start_height = point1[0]\n",
    "        start_temp = point1[1]\n",
    "        end_height = point2[0]\n",
    "        end_temp = point2[1]\n",
    "\n",
    "        # Save the data to the list\n",
    "        data.append([distance, start_height, start_temp, end_height, end_temp])\n",
    "\n",
    "\n",
    "    # clean up the data by putting it into a pandas dataframe, ranking the inversions, and parsing the inversion of interest\n",
    "    OBS_inversion_ranks = pd.DataFrame(data, columns=['Distance', 'Starting Height', 'Starting Temperature', 'Ending Height', 'Ending Temperature'])\n",
    "    OBS_inversion_ranks = OBS_inversion_ranks.sort_values(by='Distance', ascending=False)\n",
    "    OBS_inversion_ranks = OBS_inversion_ranks.reset_index(drop=True)\n",
    "    \n",
    "    # And the same for the HRRR\n",
    "    HRRR_inversions = []\n",
    "\n",
    "    # booleans to check if we started/finished grabbing an inversion\n",
    "    start_grabbed = 0\n",
    "    end_grabbed = 0\n",
    "\n",
    "    # Scan all points to find positive lapse rates\n",
    "    for i in range(len(HRRR_df_standard['lapse_rate'])):\n",
    "\n",
    "        # Positive lapse rate found! Save this coordiate as [height, temp]\n",
    "        if ( HRRR_df_standard['lapse_rate'][i] >= 0 ) and ( start_grabbed == 0):\n",
    "            start_coord = [HRRR_df_standard['Height AGL (m)'][i], HRRR_df_standard['Temp (°C)'][i]]\n",
    "            start_grabbed = 1 # we just grabbed the start point, so tell the program we did\n",
    "            end_grabbed = 0   # haven't grabbed the ending point though...\n",
    "\n",
    "        # The lapse rate above this point is no longer positive.... Guess we hit the end of the inversion\n",
    "        # Note, this part can only run if we've already started grabbing an inversion\n",
    "        elif ( HRRR_df_standard['lapse_rate'][i] < 0 ) and ( start_grabbed == 1):\n",
    "            end_coord = [HRRR_df_standard['Height AGL (m)'][i], HRRR_df_standard['Temp (°C)'][i]]\n",
    "            start_grabbed = 0 # We finished grabbing this inversion, so we can tell the program it's ready to start a new one\n",
    "            end_grabbed = 1   # Finished grabbing this inversion, so let's say that we grabbed it\n",
    "\n",
    "        # save the coordinates, ensuring that we finished grabbing this inversion and the program is ready for the next one\n",
    "        if end_grabbed == 1 and start_grabbed == 0:\n",
    "            HRRR_inversions.append([start_coord, end_coord])\n",
    "            start_grabbed = 0\n",
    "            end_grabbed = 0\n",
    "\n",
    "    # Calculate distances and store them with their respective line segments\n",
    "    data = [] # simple list to store the data of each inversion. Not organized quite yet\n",
    "\n",
    "    for line in HRRR_inversions:\n",
    "        point1, point2 = line\n",
    "        distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2) # Distance formula\n",
    "\n",
    "        # Extract relevant information\n",
    "        start_height = point1[0]\n",
    "        start_temp = point1[1]\n",
    "        end_height = point2[0]\n",
    "        end_temp = point2[1]\n",
    "\n",
    "        # Save the data to the list\n",
    "        data.append([distance, start_height, start_temp, end_height, end_temp])\n",
    "\n",
    "\n",
    "    # clean up the data by putting it into a pandas dataframe, ranking the inversions, and parsing the inversion of interest\n",
    "    HRRR_inversion_ranks = pd.DataFrame(data, columns=['Distance', 'Starting Height', 'Starting Temperature', 'Ending Height', 'Ending Temperature'])\n",
    "    HRRR_inversion_ranks = HRRR_inversion_ranks.sort_values(by='Distance', ascending=False)\n",
    "    HRRR_inversion_ranks = HRRR_inversion_ranks.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Grab the inversion lists in normalized coordinates\n",
    "    \n",
    "    \n",
    "    OBS_inversions_norm = []\n",
    "\n",
    "    # booleans to check if we started/finished grabbing an inversion\n",
    "    start_grabbed = 0\n",
    "    end_grabbed = 0\n",
    "\n",
    "    # Scan all points to find positive lapse rates\n",
    "    for i in range(len(OBS_df_normalized['lapse_rate'])):\n",
    "\n",
    "        # Positive lapse rate found! Save this coordiate as [height, temp]\n",
    "        if ( OBS_df_normalized['lapse_rate'][i] >= 0 ) and ( start_grabbed == 0):\n",
    "            start_coord = [OBS_df_normalized['Height AGL (m)'][i], OBS_df_normalized['Temp (°C)'][i]]\n",
    "            start_grabbed = 1 # we just grabbed the start point, so tell the program we did\n",
    "            end_grabbed = 0   # haven't grabbed the ending point though...\n",
    "\n",
    "        # The lapse rate above this point is no longer positive.... Guess we hit the end of the inversion\n",
    "        # Note, this part can only run if we've already started grabbing an inversion\n",
    "        elif ( OBS_df_normalized['lapse_rate'][i] < 0 ) and ( start_grabbed == 1):\n",
    "            end_coord = [OBS_df_normalized['Height AGL (m)'][i], OBS_df_normalized['Temp (°C)'][i]]\n",
    "            start_grabbed = 0 # We finished grabbing this inversion, so we can tell the program it's ready to start a new one\n",
    "            end_grabbed = 1   # Finished grabbing this inversion, so let's say that we grabbed it\n",
    "\n",
    "        # save the coordinates, ensuring that we finished grabbing this inversion and the program is ready for the next one\n",
    "        if end_grabbed == 1 and start_grabbed == 0:\n",
    "            OBS_inversions_norm.append([start_coord, end_coord])\n",
    "            start_grabbed = 0\n",
    "            end_grabbed = 0\n",
    "\n",
    "    # Calculate distances and store them with their respective line segments\n",
    "    data = [] # simple list to store the data of each inversion. Not organized quite yet\n",
    "\n",
    "    for line in OBS_inversions_norm:\n",
    "        point1, point2 = line\n",
    "        distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2) # Distance formula\n",
    "\n",
    "        # Extract relevant information\n",
    "        start_height = point1[0]\n",
    "        start_temp = point1[1]\n",
    "        end_height = point2[0]\n",
    "        end_temp = point2[1]\n",
    "\n",
    "        # Save the data to the list\n",
    "        data.append([distance, start_height, start_temp, end_height, end_temp])\n",
    "\n",
    "\n",
    "    # clean up the data by putting it into a pandas dataframe, ranking the inversions, and parsing the inversion of interest\n",
    "    OBS_inversion_ranks_norm = pd.DataFrame(data, columns=['Distance', 'Starting Height', 'Starting Temperature', 'Ending Height', 'Ending Temperature'])\n",
    "\n",
    "    OBS_inversion_ranks      = OBS_inversion_ranks.sort_values(by='Starting Height', ascending=False)\n",
    "    OBS_inversion_ranks_norm = OBS_inversion_ranks_norm.sort_values(by='Starting Height', ascending=False)\n",
    "    OBS_inversion_ranks      = OBS_inversion_ranks.reset_index(drop=True)\n",
    "    OBS_inversion_ranks_norm = OBS_inversion_ranks_norm.reset_index(drop=True)\n",
    "\n",
    "    OBS_inversion_ranks['Distance'] = OBS_inversion_ranks_norm['Distance']\n",
    "\n",
    "    OBS_inversion_ranks = OBS_inversion_ranks.sort_values(by='Distance', ascending=False)\n",
    "    OBS_inversion_ranks = OBS_inversion_ranks.reset_index(drop=True)\n",
    "    \n",
    "    # Likewise with the HRRR data\n",
    "    HRRR_inversions_norm = []\n",
    "\n",
    "    # booleans to check if we started/finished grabbing an inversion\n",
    "    start_grabbed = 0\n",
    "    end_grabbed = 0\n",
    "\n",
    "    # Scan all points to find positive lapse rates\n",
    "    for i in range(len(HRRR_df_normalized['lapse_rate'])):\n",
    "\n",
    "        # Positive lapse rate found! Save this coordiate as [height, temp]\n",
    "        if ( HRRR_df_normalized['lapse_rate'][i] >= 0 ) and ( start_grabbed == 0):\n",
    "            start_coord = [HRRR_df_normalized['Height AGL (m)'][i], HRRR_df_normalized['Temp (°C)'][i]]\n",
    "            start_grabbed = 1 # we just grabbed the start point, so tell the program we did\n",
    "            end_grabbed = 0   # haven't grabbed the ending point though...\n",
    "\n",
    "        # The lapse rate above this point is no longer positive.... Guess we hit the end of the inversion\n",
    "        # Note, this part can only run if we've already started grabbing an inversion\n",
    "        elif ( HRRR_df_normalized['lapse_rate'][i] < 0 ) and ( start_grabbed == 1):\n",
    "            end_coord = [HRRR_df_normalized['Height AGL (m)'][i], HRRR_df_normalized['Temp (°C)'][i]]\n",
    "            start_grabbed = 0 # We finished grabbing this inversion, so we can tell the program it's ready to start a new one\n",
    "            end_grabbed = 1   # Finished grabbing this inversion, so let's say that we grabbed it\n",
    "\n",
    "        # save the coordinates, ensuring that we finished grabbing this inversion and the program is ready for the next one\n",
    "        if end_grabbed == 1 and start_grabbed == 0:\n",
    "            HRRR_inversions_norm.append([start_coord, end_coord])\n",
    "            start_grabbed = 0\n",
    "            end_grabbed = 0\n",
    "\n",
    "    # Likewise with the HRRR data\n",
    "    # Calculate distances and store them with their respective line segments\n",
    "    data = [] # simple list to store the data of each inversion. Not organized quite yet\n",
    "\n",
    "    for line in HRRR_inversions_norm:\n",
    "        point1, point2 = line\n",
    "        distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2) # Distance formula\n",
    "\n",
    "        # Extract relevant information\n",
    "        start_height = point1[0]\n",
    "        start_temp = point1[1]\n",
    "        end_height = point2[0]\n",
    "        end_temp = point2[1]\n",
    "\n",
    "        # Save the data to the list\n",
    "        data.append([distance, start_height, start_temp, end_height, end_temp])\n",
    "\n",
    "\n",
    "    # clean up the data by putting it into a pandas dataframe, ranking the inversions, and parsing the inversion of interest\n",
    "    HRRR_inversion_ranks_norm = pd.DataFrame(data, columns=['Distance', 'Starting Height', 'Starting Temperature', 'Ending Height', 'Ending Temperature'])\n",
    "\n",
    "    HRRR_inversion_ranks      = HRRR_inversion_ranks.sort_values(by='Starting Height', ascending=False)\n",
    "    HRRR_inversion_ranks_norm = HRRR_inversion_ranks_norm.sort_values(by='Starting Height', ascending=False)\n",
    "    HRRR_inversion_ranks      = HRRR_inversion_ranks.reset_index(drop=True)\n",
    "    HRRR_inversion_ranks_norm = HRRR_inversion_ranks_norm.reset_index(drop=True)\n",
    "\n",
    "    HRRR_inversion_ranks['Distance'] = HRRR_inversion_ranks_norm['Distance']\n",
    "\n",
    "    HRRR_inversion_ranks = HRRR_inversion_ranks.sort_values(by='Distance', ascending=False)\n",
    "    HRRR_inversion_ranks = HRRR_inversion_ranks.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Save the primary inversions and return them\n",
    "    OBS_prime  = OBS_inversion_ranks.head(1)\n",
    "    HRRR_prime = HRRR_inversion_ranks.head(1)\n",
    "    \n",
    "    return OBS_prime, HRRR_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f9c8b",
   "metadata": {},
   "source": [
    "## Function to determine the successful creation of a modelled temperature inversion\n",
    "\n",
    "Now that we have the primary inversion of interest in both the modelled and observed data, we need to check if the HRRR correctly procuded an inversion (Successful Inversions), correctly left an inversion out (No Inversions), incorrectly produced an inversion where there should have been none (False Positives), or incorrectly left an inversion out (False Negatives). The criteria for each is listed below:\n",
    "\n",
    "### Criteria for success category\n",
    "* Successful Inversions\n",
    " * A temperature inversion vector must be present in both the modelled and observed data **AND**\n",
    " * The height domains of both the modelled and observed inversion vectors must overlap\n",
    "* No Inversions\n",
    " * There must be zero inversions detected in both the modelled and observed data\n",
    "* False Positives\n",
    " * The modelled data has an inversion vector while the observed data has none **OR**\n",
    " * Both the modelled and observed data have inversion vectors but their height domains do not overlap\n",
    "* False Negatives\n",
    " * The observed data has an inversion vector while the modelled data has none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5095e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correctness of the HRRR\n",
    "def category(OBS_prime, HRRR_prime):\n",
    "    \n",
    "    # First check if either primary inversion vector dataframe is empty\n",
    "    # find false positives or false negatives\n",
    "    if (len(OBS_prime) != 0) and (len(HRRR_prime) == 0):\n",
    "        return 'False Negative'\n",
    "    \n",
    "    elif (len(OBS_prime) == 0) and (len(HRRR_prime) != 0):\n",
    "        return 'False Positive'\n",
    "    \n",
    "    elif (len(OBS_prime) == 0) and (len(HRRR_prime) == 0):\n",
    "        return 'No inversions'\n",
    "    \n",
    "    # Now check if the heights overlap\n",
    "    else:\n",
    "        # Height domain\n",
    "        h_min = HRRR_prime['Starting Height'].values[0]\n",
    "        h_max = HRRR_prime['Ending Height'].values[0]\n",
    "        \n",
    "        # filter out inversions whose domain does not overlap with the primary HRRR inversion\n",
    "        OBS_filtered = OBS_prime[(OBS_prime['Ending Height'] >= h_min) & (OBS_prime['Starting Height'] <= h_max)]\n",
    "        \n",
    "        # if we filtered out all the inversions, meaning none overlapped with the HRRR inversion, then we can assume the HRRR\n",
    "        # did not properly detect a legitamate inversion\n",
    "        if len(OBS_filtered) == 0:\n",
    "            return 'False Positive'\n",
    "        else:\n",
    "            return 'Successful Inversions'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913bfe8",
   "metadata": {},
   "source": [
    "## Run through all dates and stations\n",
    "\n",
    "Now that we have a way to categorize if the HRRR correctly modelled an inversion for any given sounding, we can iterate through all our stations and dates to get percentages on the HRRR's correctness. We'll save this information into a csv file so that we can represent the data in a contingency table through the Microsoft Office environment. I did this simply because I prefer the way it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a total results counter:\n",
    "total_entries_all                = []\n",
    "false_positive_counter_all       = []\n",
    "false_negative_counter_all       = []\n",
    "no_inversion_counter_all         = []\n",
    "successful_inversion_counter_all = []\n",
    "perc_succ_all                    = []\n",
    "perc_succ_inversions_all         = []\n",
    "\n",
    "\n",
    "# Iterate through each station\n",
    "for i in range(len(stids_HRRR)):\n",
    "    \n",
    "    # Start a results counter for the current station\n",
    "    false_positive_counter       = 0\n",
    "    false_negative_counter       = 0\n",
    "    no_inversion_counter         = 0\n",
    "    successful_inversion_counter = 0\n",
    "    \n",
    "    \n",
    "    # iterate through each date\n",
    "    curr_time = start_time\n",
    "    while curr_time <= stop_time:\n",
    "        \n",
    "        date_str = datetime.strftime(curr_time, \"%Y-%m-%d %H:%M\")\n",
    "        \n",
    "        # Make 10 attempts to grab the modelled dataframe. If 10 attempts fail, we'll pass over this date\n",
    "        grabbed = False\n",
    "        for j in range(10):\n",
    "            print('Grabbing HRRR data for '+stids_HRRR[i]+'. ATTEMPT '+str(j+1)+'/10' )\n",
    "            HRRR_df = mk_HRRR_df(date_str, stids_HRRR[i], sounding_dir)\n",
    "            \n",
    "            if type(HRRR_df) == str:\n",
    "                if HRRR_df == 'nan':\n",
    "                    print('Failed to grab HRRR data. Retrying....')\n",
    "            elif isinstance(HRRR_df, pd.DataFrame):\n",
    "                grabbed = True\n",
    "                break\n",
    "            else:\n",
    "                print('Failed to grab HRRR data. Retrying....')\n",
    "        \n",
    "        if not grabbed:\n",
    "            print('Failed to grab HRRR data for '+date_str+'. Skipping....')\n",
    "            # jump to next date\n",
    "            curr_time = curr_time + timedelta(hours=12)\n",
    "            continue\n",
    "        \n",
    "        # Do the same for the observed data\n",
    "        grabbed = False\n",
    "        for j in range(10):\n",
    "            print('Grabbing OBS data for '+stids_HRRR[i]+'. ATTEMPT '+str(j+1)+'/10' )\n",
    "            OBS_df = get_OBS_data(date_str, stids_HRRR[i], stids_OBS[i], sounding_dir)\n",
    "            \n",
    "            if type(OBS_df) == str:\n",
    "                if OBS_df == 'nan':\n",
    "                    print('Failed to grab OBS data. Retrying....')\n",
    "            elif isinstance(OBS_df, pd.DataFrame):\n",
    "                grabbed = True\n",
    "                break\n",
    "            else:\n",
    "                print('Failed to grab OBS data. Retrying....')\n",
    "        \n",
    "        if not grabbed:\n",
    "            print('Failed to grab OBS data for '+date_str+'. Skipping....')\n",
    "            # jump to next date\n",
    "            curr_time = curr_time + timedelta(hours=12)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Now we should have a dataframe for each. Let's now get the primary inversions if any exist\n",
    "        # Fix the starting heights\n",
    "        OBS_df, HRRR_df = start_at_same_height(OBS_df, HRRR_df)\n",
    "        \n",
    "        # Convert to standard and normalized height coordinates\n",
    "        OBS_df_standard, HRRR_df_standard = mk_height_df(OBS_df, HRRR_df)\n",
    "        OBS_df_normalized, HRRR_df_normalized = mk_normalized_height_df(OBS_df_standard, HRRR_df_standard)\n",
    "        \n",
    "        # get the primary inversions\n",
    "        OBS_prime, HRRR_prime = inversion_grabber(OBS_df_standard, OBS_df_normalized, HRRR_df_standard, HRRR_df_normalized)\n",
    "        \n",
    "        # Finally, let's get our result\n",
    "        result = category(OBS_prime, HRRR_prime)\n",
    "        \n",
    "        if result == 'False Negative':\n",
    "            false_negative_counter = false_negative_counter+1\n",
    "        elif result == 'False Positive':\n",
    "            false_positive_counter = false_positive_counter+1\n",
    "        elif result == 'No inversions':\n",
    "            no_inversion_counter = no_inversion_counter+1\n",
    "        elif result == 'Successful Inversions':\n",
    "            successful_inversion_counter = successful_inversion_counter+1\n",
    "        else:\n",
    "            print('FAILED TO DETERMINE THE RESULT!!!!!!!')\n",
    "        \n",
    "        \n",
    "        # jump to next date\n",
    "        curr_time = curr_time + timedelta(hours=12)\n",
    "        \n",
    "    # Calculate the correctness\n",
    "    total_inversions = successful_inversion_counter + false_negative_counter\n",
    "    total_entries    = total_inversions + false_positive_counter + no_inversion_counter\n",
    "    \n",
    "    # determine the percent correctness\n",
    "    perc_succ            = ( float(successful_inversion_counter + no_inversion_counter) / float(total_entries) ) * 100\n",
    "    perc_succ = round(perc_succ, 2)\n",
    "    perc_succ_inversions = ( float(successful_inversion_counter) / float(total_inversions) ) * 100\n",
    "    perc_succ_inversions = round(perc_succ_inversions, 2)\n",
    "    \n",
    "    # save the result to memory\n",
    "    total_entries_all.append(total_entries)\n",
    "    successful_inversion_counter_all.append(successful_inversion_counter)\n",
    "    no_inversion_counter_all.append(no_inversion_counter)\n",
    "    false_negative_counter_all.append(false_negative_counter)\n",
    "    false_positive_counter_all.append(false_positive_counter)\n",
    "    perc_succ_all.append(perc_succ)\n",
    "    perc_succ_inversions_all.append(perc_succ_inversions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b60762",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**DUE TO MEMORY ALLOCATION ISSUES, OUTPUT FROM THIS POINT ONWARD IS NOT INCLUDED**</span>.\n",
    "\n",
    "A supplemental python program built from this notebook is provided to be used outside of a web browser. This helps greatly with memory allocation\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d886504",
   "metadata": {},
   "source": [
    "## Save the data to a csv file\n",
    "\n",
    "Now that we have the data saved to memory, let's save it to a csv file to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results for easy reading within jupyter\n",
    "for i in range(len(stids_HRRR)):\n",
    "    \n",
    "    total_entries = successful_inversion_counter_all[i] + no_inversion_counter_all[i] + false_negative_counter_all[i] + false_positive_counter_all[i]\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print('########################')\n",
    "    print('### RESULTS FOR '+stids_HRRR[i]+' ###')\n",
    "    print('########################')\n",
    "    print('\\n')\n",
    "    \n",
    "    print(f\"Number of rows checked: {total_entries}\")\n",
    "    print(f\"False Positives: {false_positive_counter_all[i]}\")\n",
    "    print(f\"False Negatives: {false_negative_counter_all[i]}\")\n",
    "    print(f\"No Inversion: {no_inversion_counter_all[i]}\")\n",
    "    print(f\"Successful Inversions: {successful_inversion_counter_all[i]}\")\n",
    "    print(f\"\\nPercent Correct (overall): {perc_succ_all[i]}%\")\n",
    "    print(f\"Percent Correct (inversions only): {perc_succ_inversions_all[i]}%\")\n",
    "\n",
    "    \n",
    "    \n",
    "# save the data\n",
    "filename = 'correctness_results.csv'\n",
    "\n",
    "if not os.path.exists('./'+filename):\n",
    "    \n",
    "    # start the file\n",
    "    header = ['Cutoff Height (m)', '',\n",
    "              'TOTAL - Total number of soundings', 'TOTAL - Successful inversion count', 'TOTAL - No inversion count', 'TOTAL - False positive count', 'TOTAL - False negative count', 'TOTAL - Percent Successful (overall)', 'TOTAL - Percent Successful (inversions only)', '',\n",
    "              'KUNR - Total number of soundings', 'KUNR - Successful inversion count', 'KUNR - No inversion count', 'KUNR - False positive count', 'KUNR - False negative count', 'KUNR - Percent Successful (overall)', 'KUNR - Percent Successful (inversions only)', '',\n",
    "              'KRIW - Total number of soundings', 'KRIW - Successful inversion count', 'KRIW - No inversion count', 'KRIW - False positive count', 'KRIW - False negative count', 'KRIW - Percent Successful (overall)', 'KRIW - Percent Successful (inversions only)', '',\n",
    "              'KGGW - Total number of soundings', 'KGGW - Successful inversion count', 'KGGW - No inversion count', 'KGGW - False positive count', 'KGGW - False negative count', 'KGGW - Percent Successful (overall)', 'KGGW - Percent Successful (inversions only)', '',\n",
    "              'KDNR - Total number of soundings', 'KDNR - Successful inversion count', 'KDNR - No inversion count', 'KDNR - False positive count', 'KDNR - False negative count', 'KDNR - Percent Successful (overall)', 'KDNR - Percent Successful (inversions only)', ''\n",
    "             ]\n",
    "    \n",
    "    with open('./'+filename, 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(header)\n",
    "    \n",
    "    \n",
    "# get total stats\n",
    "false_positive_counter       = 0\n",
    "false_negative_counter       = 0\n",
    "no_inversion_counter         = 0\n",
    "successful_inversion_counter = 0\n",
    "\n",
    "for i in range(len(perc_succ_all)):\n",
    "    false_positive_counter       = false_positive_counter + false_positive_counter_all[i]\n",
    "    false_negative_counter       = false_negative_counter + false_negative_counter_all[i]\n",
    "    no_inversion_counter         = no_inversion_counter + no_inversion_counter_all[i]\n",
    "    successful_inversion_counter = successful_inversion_counter + successful_inversion_counter_all[i]\n",
    "\n",
    "# Calculate the correctness\n",
    "total_inversions = successful_inversion_counter + false_negative_counter\n",
    "total_entries    = total_inversions + false_positive_counter + no_inversion_counter\n",
    "\n",
    "# determine the percent correctness\n",
    "perc_succ            = ( float(successful_inversion_counter + no_inversion_counter) / float(total_entries) ) * 100\n",
    "perc_succ = round(perc_succ, 2)\n",
    "perc_succ_inversions = ( float(successful_inversion_counter) / float(total_inversions) ) * 100\n",
    "perc_succ_inversions = round(perc_succ_inversions, 2)\n",
    "\n",
    "# print the results\n",
    "\n",
    "print('\\n\\n')\n",
    "print('################################')\n",
    "print('### RESULTS FOR ALL STATIONS ###')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "print(f\"Number of rows checked: {total_entries}\")\n",
    "print(f\"False Positives: {false_positive_counter}\")\n",
    "print(f\"False Negatives: {false_negative_counter}\")\n",
    "print(f\"No Inversion: {no_inversion_counter}\")\n",
    "print(f\"Successful Inversions: {successful_inversion_counter}\")\n",
    "print(f\"\\nPercent Correct (overall): {perc_succ}%\")\n",
    "print(f\"Percent Correct (inversions only): {perc_succ_inversions}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the data\n",
    "row = [5000, '',\n",
    "      total_entries, successful_inversion_counter, no_inversion_counter, false_positive_counter, false_negative_counter, perc_succ, perc_succ_inversions, '',\n",
    "\n",
    "      total_entries_all[0], successful_inversion_counter_all[0], no_inversion_counter_all[0], false_positive_counter_all[0], false_negative_counter_all[0], perc_succ_all[0], perc_succ_inversions_all[0], '',\n",
    "      total_entries_all[1], successful_inversion_counter_all[1], no_inversion_counter_all[1], false_positive_counter_all[1], false_negative_counter_all[1], perc_succ_all[1], perc_succ_inversions_all[1], '',\n",
    "      total_entries_all[2], successful_inversion_counter_all[2], no_inversion_counter_all[2], false_positive_counter_all[2], false_negative_counter_all[2], perc_succ_all[2], perc_succ_inversions_all[2], '',\n",
    "      total_entries_all[3], successful_inversion_counter_all[3], no_inversion_counter_all[3], false_positive_counter_all[3], false_negative_counter_all[3], perc_succ_all[3], perc_succ_inversions_all[3], '']\n",
    "\n",
    "with open('./'+filename, 'a', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13313896",
   "metadata": {},
   "source": [
    "## Making improvements to model correctness\n",
    "\n",
    "Now that we have a metric for how well the HRRR is doing at modelling the existence of temperature inversions, let's now think about how we can hone in on impactful temperature inversions and see if we can improve the model correctness. Of the types of temperature inversions that are impactful for day-to-day weather, surface-inversions hold a particular amount of weight. With that, it begs the question: If the height domain for which we restrict our soundings to is reduced, what impact will that have on the HRRR model correctness? Let's find out by iteratively reducing our height domain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da413e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a cutoff height we haven't done yet\n",
    "cutoff = 4900\n",
    "\n",
    "# iteratively reduce the cutoff height. From trial and error, going below 500m produces too few inversions to be worthwhile\n",
    "while cutoff >= 500:\n",
    "    \n",
    "    # get stats for each station\n",
    "    # Start a total results counter:\n",
    "    total_entries_all                = []\n",
    "    false_positive_counter_all       = []\n",
    "    false_negative_counter_all       = []\n",
    "    no_inversion_counter_all         = []\n",
    "    successful_inversion_counter_all = []\n",
    "    perc_succ_all                    = []\n",
    "    perc_succ_inversions_all         = []\n",
    "\n",
    "\n",
    "    # Iterate through each station\n",
    "    for i in range(len(stids_HRRR)):\n",
    "\n",
    "        # Start a results counter for the current station\n",
    "        false_positive_counter       = 0\n",
    "        false_negative_counter       = 0\n",
    "        no_inversion_counter         = 0\n",
    "        successful_inversion_counter = 0\n",
    "\n",
    "\n",
    "        # iterate through each date\n",
    "        curr_time = start_time\n",
    "        while curr_time <= stop_time:\n",
    "\n",
    "            date_str = datetime.strftime(curr_time, \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "            # Make 10 attempts to grab the modelled dataframe. If 10 attempts fail, we'll pass over this date\n",
    "            grabbed = False\n",
    "            for j in range(10):\n",
    "                print('Grabbing HRRR data for '+stids_HRRR[i]+'. ATTEMPT '+str(j+1)+'/10' )\n",
    "                HRRR_df = mk_HRRR_df(date_str, stids_HRRR[i], sounding_dir)\n",
    "\n",
    "                if type(HRRR_df) == str:\n",
    "                    if HRRR_df == 'nan':\n",
    "                        print('Failed to grab HRRR data. Retrying....')\n",
    "                elif isinstance(HRRR_df, pd.DataFrame):\n",
    "                    grabbed = True\n",
    "                    break\n",
    "                else:\n",
    "                    print('Failed to grab HRRR data. Retrying....')\n",
    "\n",
    "            if not grabbed:\n",
    "                print('Failed to grab HRRR data for '+date_str+'. Skipping....')\n",
    "                # jump to next date\n",
    "                curr_time = curr_time + timedelta(hours=12)\n",
    "                continue\n",
    "\n",
    "            # Do the same for the observed data\n",
    "            grabbed = False\n",
    "            for j in range(10):\n",
    "                print('Grabbing OBS data for '+stids_HRRR[i]+'. ATTEMPT '+str(j+1)+'/10' )\n",
    "                OBS_df = get_OBS_data(date_str, stids_HRRR[i], stids_OBS[i], sounding_dir)\n",
    "\n",
    "                if type(OBS_df) == str:\n",
    "                    if OBS_df == 'nan':\n",
    "                        print('Failed to grab OBS data. Retrying....')\n",
    "                elif isinstance(OBS_df, pd.DataFrame):\n",
    "                    grabbed = True\n",
    "                    break\n",
    "                else:\n",
    "                    print('Failed to grab OBS data. Retrying....')\n",
    "\n",
    "            if not grabbed:\n",
    "                print('Failed to grab OBS data for '+date_str+'. Skipping....')\n",
    "                # jump to next date\n",
    "                curr_time = curr_time + timedelta(hours=12)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Now we should have a dataframe for each. Let's now get the primary inversions if any exist\n",
    "            # Fix the starting heights\n",
    "            OBS_df, HRRR_df = start_at_same_height(OBS_df, HRRR_df)\n",
    "\n",
    "            # Convert to standard and normalized height coordinates\n",
    "            OBS_df_standard, HRRR_df_standard = mk_height_df(OBS_df, HRRR_df, cutoff=cutoff)\n",
    "            OBS_df_normalized, HRRR_df_normalized = mk_normalized_height_df(OBS_df_standard, HRRR_df_standard)\n",
    "\n",
    "            # get the primary inversions\n",
    "            OBS_prime, HRRR_prime = inversion_grabber(OBS_df_standard, OBS_df_normalized, HRRR_df_standard, HRRR_df_normalized)\n",
    "\n",
    "            # Finally, let's get our result\n",
    "            result = category(OBS_prime, HRRR_prime)\n",
    "\n",
    "            if result == 'False Negative':\n",
    "                false_negative_counter = false_negative_counter+1\n",
    "            elif result == 'False Positive':\n",
    "                false_positive_counter = false_positive_counter+1\n",
    "            elif result == 'No inversions':\n",
    "                no_inversion_counter = no_inversion_counter+1\n",
    "            elif result == 'Successful Inversions':\n",
    "                successful_inversion_counter = successful_inversion_counter+1\n",
    "            else:\n",
    "                print('FAILED TO DETERMINE THE RESULT!!!!!!!')\n",
    "\n",
    "\n",
    "            # jump to next date\n",
    "            curr_time = curr_time + timedelta(hours=12)\n",
    "\n",
    "        # Calculate the correctness\n",
    "        total_inversions = successful_inversion_counter + false_negative_counter\n",
    "        total_entries    = total_inversions + false_positive_counter + no_inversion_counter\n",
    "\n",
    "        # determine the percent correctness\n",
    "        perc_succ            = ( float(successful_inversion_counter + no_inversion_counter) / float(total_entries) ) * 100\n",
    "        perc_succ = round(perc_succ, 2)\n",
    "        perc_succ_inversions = ( float(successful_inversion_counter) / float(total_inversions) ) * 100\n",
    "        perc_succ_inversions = round(perc_succ_inversions, 2)\n",
    "\n",
    "        # save the result to memory\n",
    "        total_entries_all.append(total_entries)\n",
    "        successful_inversion_counter_all.append(successful_inversion_counter)\n",
    "        no_inversion_counter_all.append(no_inversion_counter)\n",
    "        false_negative_counter_all.append(false_negative_counter)\n",
    "        false_positive_counter_all.append(false_positive_counter)\n",
    "        perc_succ_all.append(perc_succ)\n",
    "        perc_succ_inversions_all.append(perc_succ_inversions)\n",
    "    \n",
    "    \n",
    "    # get total stats\n",
    "    false_positive_counter       = 0\n",
    "    false_negative_counter       = 0\n",
    "    no_inversion_counter         = 0\n",
    "    successful_inversion_counter = 0\n",
    "\n",
    "    for i in range(len(perc_succ_all)):\n",
    "        false_positive_counter       = false_positive_counter + false_positive_counter_all[i]\n",
    "        false_negative_counter       = false_negative_counter + false_negative_counter_all[i]\n",
    "        no_inversion_counter         = no_inversion_counter + no_inversion_counter_all[i]\n",
    "        successful_inversion_counter = successful_inversion_counter + successful_inversion_counter_all[i]\n",
    "\n",
    "    # Calculate the correctness\n",
    "    total_inversions = successful_inversion_counter + false_negative_counter\n",
    "    total_entries    = total_inversions + false_positive_counter + no_inversion_counter\n",
    "\n",
    "    # determine the percent correctness\n",
    "    perc_succ            = ( float(successful_inversion_counter + no_inversion_counter) / float(total_entries) ) * 100\n",
    "    perc_succ = round(perc_succ, 2)\n",
    "    perc_succ_inversions = ( float(successful_inversion_counter) / float(total_inversions) ) * 100\n",
    "    perc_succ_inversions = round(perc_succ_inversions, 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Save the data\n",
    "    row = [cutoff, '',\n",
    "          total_entries, successful_inversion_counter, no_inversion_counter, false_positive_counter, false_negative_counter, perc_succ, perc_succ_inversions, '',\n",
    "\n",
    "          total_entries_all[0], successful_inversion_counter_all[0], no_inversion_counter_all[0], false_positive_counter_all[0], false_negative_counter_all[0], perc_succ_all[0], perc_succ_inversions_all[0], '',\n",
    "          total_entries_all[1], successful_inversion_counter_all[1], no_inversion_counter_all[1], false_positive_counter_all[1], false_negative_counter_all[1], perc_succ_all[1], perc_succ_inversions_all[1], '',\n",
    "          total_entries_all[2], successful_inversion_counter_all[2], no_inversion_counter_all[2], false_positive_counter_all[2], false_negative_counter_all[2], perc_succ_all[2], perc_succ_inversions_all[2], '',\n",
    "          total_entries_all[3], successful_inversion_counter_all[3], no_inversion_counter_all[3], false_positive_counter_all[3], false_negative_counter_all[3], perc_succ_all[3], perc_succ_inversions_all[3], '']\n",
    "\n",
    "    with open('./'+filename, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row)\n",
    "        \n",
    "    cutoff = cutoff - 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990394c0",
   "metadata": {},
   "source": [
    "Our results are provided in the attached correctness_results.csv file. Looking carefully, we see that as the cutoff height is reduced, the correctness of the HRRR at modelling inversions improves, which would imply that the HRRR is best at predicting the existence of surface-based inversions\n",
    "\n",
    "However, we can improve the model further. The purpose behind this project is to be used in another larger project where  HRRR data will be used in lieu of upper air data where none exist. If the HRRR is most capable at modelling the existence of inversions at or near the surface, would it be beneficial to use actual ground-based observations as a stand-in for modelled surface conditions? If the HRRR is being used in lieu of observations, it may be best to insert actual observations that can be readily obtained.\n",
    "\n",
    "We will explore this idea and see how model correctness improves by replacing the surface temperature in HRRR modelled data with observed temperature data from mesonet stations nearby each site. Note: unfortunately, I could not find a python library in a reasonable amount of time that was able to grab nearby mesonet station data. Therefore, mesonet data was manually downloaded from the [Synoptic Data - Data Download Tool](https://download.synopticdata.com/). A map of stations available can be found via [this link](https://www.wrh.noaa.gov/map/?obs=true&basemap=OpenStreetMap&boundaries=true,false&obs_popup=true)\n",
    "\n",
    "The following mesonet stations were used for each station:\n",
    "* KRAP: Rapid City WFO - KUNR. Does not go past 2022-07-20 00:00\n",
    "* KRIW: Riverton Regional Airport - KRIW\n",
    "* KGGW: Glasgow International Airport - KGGW\n",
    "* KDNR: Urban Farm Precipitation - URBC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b37197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup mesonet station directories, in line with the stids_HRRR and stids_OBS lists\n",
    "mesonet_stids = ['KUNR', 'KRIW', 'KGGW', 'URBC2']\n",
    "mesonet_dir   = './mesonet_csvs/'\n",
    "\n",
    "# Grab Mesonet Data\n",
    "\n",
    "mesonet_dfs = []\n",
    "\n",
    "for stid in mesonet_stids:\n",
    "    dates = []\n",
    "    temps = []\n",
    "\n",
    "    mesonet_csv = mesonet_dir+stid+'.csv'\n",
    "\n",
    "    with open(mesonet_csv) as f:\n",
    "        data = f.read()\n",
    "\n",
    "    #parse out the header\n",
    "    lines = data.split(\"\\n\")    \n",
    "    header = lines[10].split(\",\")\n",
    "    lines = lines[12:]\n",
    "\n",
    "    for i in range(len(lines) - 1):\n",
    "        values = lines[i].split(\",\")\n",
    "\n",
    "        if values[1] != '':\n",
    "            dates.append(datetime.strptime(values[1], \"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "\n",
    "            if values[2] != '':\n",
    "                temps.append(float(values[2]))\n",
    "            else:\n",
    "                temps.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "    mesonet_data = pd.DataFrame({\n",
    "        'Dates': dates,\n",
    "        'Temperature': temps\n",
    "    })\n",
    "    mesonet_data = mesonet_data.dropna(subset=['Temperature'])\n",
    "    mesonet_data = mesonet_data.reset_index(drop=True)\n",
    "    \n",
    "    mesonet_dfs.append(mesonet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c4ec6",
   "metadata": {},
   "source": [
    "Now run through all the dates again, replacing the HRRR surface value with the mesonet value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new csv file\n",
    "filename = 'correctness_results_mesonet_applied.csv'\n",
    "\n",
    "if not os.path.exists('./'+filename):\n",
    "    \n",
    "    # start the file\n",
    "    header = ['Cutoff Height (m)', '',\n",
    "              'TOTAL - Total number of soundings', 'TOTAL - Successful inversion count', 'TOTAL - No inversion count', 'TOTAL - False positive count', 'TOTAL - False negative count', 'TOTAL - Percent Successful (overall)', 'TOTAL - Percent Successful (inversions only)', '',\n",
    "              'KUNR - Total number of soundings', 'KUNR - Successful inversion count', 'KUNR - No inversion count', 'KUNR - False positive count', 'KUNR - False negative count', 'KUNR - Percent Successful (overall)', 'KUNR - Percent Successful (inversions only)', '',\n",
    "              'KRIW - Total number of soundings', 'KRIW - Successful inversion count', 'KRIW - No inversion count', 'KRIW - False positive count', 'KRIW - False negative count', 'KRIW - Percent Successful (overall)', 'KRIW - Percent Successful (inversions only)', '',\n",
    "              'KGGW - Total number of soundings', 'KGGW - Successful inversion count', 'KGGW - No inversion count', 'KGGW - False positive count', 'KGGW - False negative count', 'KGGW - Percent Successful (overall)', 'KGGW - Percent Successful (inversions only)', '',\n",
    "              'KDNR - Total number of soundings', 'KDNR - Successful inversion count', 'KDNR - No inversion count', 'KDNR - False positive count', 'KDNR - False negative count', 'KDNR - Percent Successful (overall)', 'KDNR - Percent Successful (inversions only)', ''\n",
    "             ]\n",
    "    \n",
    "    with open('./'+filename, 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(header)\n",
    "\n",
    "# set the starting cutoff height\n",
    "cutoff = 5000\n",
    "\n",
    "# iteratively reduce the cutoff height. From trial and error, going below 500m produces too few inversions to be worthwhile\n",
    "while cutoff >= 500:\n",
    "    \n",
    "    # get stats for each station\n",
    "    # Start a total results counter:\n",
    "    total_entries_all                = []\n",
    "    false_positive_counter_all       = []\n",
    "    false_negative_counter_all       = []\n",
    "    no_inversion_counter_all         = []\n",
    "    successful_inversion_counter_all = []\n",
    "    perc_succ_all                    = []\n",
    "    perc_succ_inversions_all         = []\n",
    "\n",
    "\n",
    "    # Iterate through each station\n",
    "    for i in range(len(stids_HRRR)):\n",
    "\n",
    "        # Start a results counter for the current station\n",
    "        false_positive_counter       = 0\n",
    "        false_negative_counter       = 0\n",
    "        no_inversion_counter         = 0\n",
    "        successful_inversion_counter = 0\n",
    "\n",
    "\n",
    "        # iterate through each date\n",
    "        curr_time = start_time\n",
    "        while curr_time <= stop_time:\n",
    "\n",
    "            date_str = datetime.strftime(curr_time, \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "            # Make 10 attempts to grab the modelled dataframe. If 10 attempts fail, we'll pass over this date\n",
    "            grabbed = False\n",
    "            for j in range(10):\n",
    "                print('Grabbing HRRR data for '+stids_HRRR[i]+'. ATTEMPT '+str(j+1)+'/10' )\n",
    "                HRRR_df = mk_HRRR_df(date_str, stids_HRRR[i], sounding_dir)\n",
    "\n",
    "                if type(HRRR_df) == str:\n",
    "                    if HRRR_df == 'nan':\n",
    "                        print('Failed to grab HRRR data. Retrying....')\n",
    "                elif isinstance(HRRR_df, pd.DataFrame):\n",
    "                    grabbed = True\n",
    "                    break\n",
    "                else:\n",
    "                    print('Failed to grab HRRR data. Retrying....')\n",
    "\n",
    "            if not grabbed:\n",
    "                print('Failed to grab HRRR data for '+date_str+'. Skipping....')\n",
    "                # jump to next date\n",
    "                curr_time = curr_time + timedelta(hours=12)\n",
    "                continue\n",
    "\n",
    "            # Do the same for the observed data\n",
    "            grabbed = False\n",
    "            for j in range(10):\n",
    "                print('Grabbing OBS data for '+stids_HRRR[i]+'. ATTEMPT '+str(j+1)+'/10' )\n",
    "                OBS_df = get_OBS_data(date_str, stids_HRRR[i], stids_OBS[i], sounding_dir)\n",
    "\n",
    "                if type(OBS_df) == str:\n",
    "                    if OBS_df == 'nan':\n",
    "                        print('Failed to grab OBS data. Retrying....')\n",
    "                elif isinstance(OBS_df, pd.DataFrame):\n",
    "                    grabbed = True\n",
    "                    break\n",
    "                else:\n",
    "                    print('Failed to grab OBS data. Retrying....')\n",
    "\n",
    "            if not grabbed:\n",
    "                print('Failed to grab OBS data for '+date_str+'. Skipping....')\n",
    "                # jump to next date\n",
    "                curr_time = curr_time + timedelta(hours=12)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Now we should have a dataframe for each. Let's now get the primary inversions if any exist\n",
    "            # Fix the starting heights\n",
    "            OBS_df, HRRR_df = start_at_same_height(OBS_df, HRRR_df)\n",
    "            \n",
    "            #####################################################\n",
    "            ###########     APPLYING MESONET DATA     ###########\n",
    "            #####################################################\n",
    "            \n",
    "            # Grab the data\n",
    "            mesonet_df = mesonet_dfs[i]\n",
    "            sfc_temp = None\n",
    "\n",
    "            # if the closest time in the mesonet data exceeds 1 hour, just consider that a failure\n",
    "            max_time_difference = timedelta(hours=1)\n",
    "\n",
    "            # Find the closest date\n",
    "            closest_date = mesonet_df.iloc[(mesonet_df['Dates'] - curr_time).abs().idxmin()]['Dates']\n",
    "\n",
    "            # Check if the time difference is within the specified limit\n",
    "            if abs(closest_date - curr_time) < max_time_difference:\n",
    "                sfc_temp = mesonet_df.iloc[(mesonet_df['Dates'] - curr_time).abs().idxmin()]['Temperature']\n",
    "\n",
    "            # replace the sfc value on the HRRR if we found one in the mesonet data\n",
    "            if sfc_temp is not None:\n",
    "                HRRR_df['Temp (°C)'].iloc[0] = sfc_temp\n",
    "                \n",
    "                \n",
    "            # Convert to standard and normalized height coordinates\n",
    "            OBS_df_standard, HRRR_df_standard = mk_height_df(OBS_df, HRRR_df, cutoff=cutoff)\n",
    "            OBS_df_normalized, HRRR_df_normalized = mk_normalized_height_df(OBS_df_standard, HRRR_df_standard)\n",
    "\n",
    "            # get the primary inversions\n",
    "            OBS_prime, HRRR_prime = inversion_grabber(OBS_df_standard, OBS_df_normalized, HRRR_df_standard, HRRR_df_normalized)\n",
    "\n",
    "            # Finally, let's get our result\n",
    "            result = category(OBS_prime, HRRR_prime)\n",
    "\n",
    "            if result == 'False Negative':\n",
    "                false_negative_counter = false_negative_counter+1\n",
    "            elif result == 'False Positive':\n",
    "                false_positive_counter = false_positive_counter+1\n",
    "            elif result == 'No inversions':\n",
    "                no_inversion_counter = no_inversion_counter+1\n",
    "            elif result == 'Successful Inversions':\n",
    "                successful_inversion_counter = successful_inversion_counter+1\n",
    "            else:\n",
    "                print('FAILED TO DETERMINE THE RESULT!!!!!!!')\n",
    "\n",
    "\n",
    "            # jump to next date\n",
    "            curr_time = curr_time + timedelta(hours=12)\n",
    "\n",
    "        # Calculate the correctness\n",
    "        total_inversions = successful_inversion_counter + false_negative_counter\n",
    "        total_entries    = total_inversions + false_positive_counter + no_inversion_counter\n",
    "\n",
    "        # determine the percent correctness\n",
    "        perc_succ            = ( float(successful_inversion_counter + no_inversion_counter) / float(total_entries) ) * 100\n",
    "        perc_succ = round(perc_succ, 2)\n",
    "        perc_succ_inversions = ( float(successful_inversion_counter) / float(total_inversions) ) * 100\n",
    "        perc_succ_inversions = round(perc_succ_inversions, 2)\n",
    "\n",
    "        # save the result to memory\n",
    "        total_entries_all.append(total_entries)\n",
    "        successful_inversion_counter_all.append(successful_inversion_counter)\n",
    "        no_inversion_counter_all.append(no_inversion_counter)\n",
    "        false_negative_counter_all.append(false_negative_counter)\n",
    "        false_positive_counter_all.append(false_positive_counter)\n",
    "        perc_succ_all.append(perc_succ)\n",
    "        perc_succ_inversions_all.append(perc_succ_inversions)\n",
    "    \n",
    "    \n",
    "    # get total stats\n",
    "    false_positive_counter       = 0\n",
    "    false_negative_counter       = 0\n",
    "    no_inversion_counter         = 0\n",
    "    successful_inversion_counter = 0\n",
    "\n",
    "    for i in range(len(perc_succ_all)):\n",
    "        false_positive_counter       = false_positive_counter + false_positive_counter_all[i]\n",
    "        false_negative_counter       = false_negative_counter + false_negative_counter_all[i]\n",
    "        no_inversion_counter         = no_inversion_counter + no_inversion_counter_all[i]\n",
    "        successful_inversion_counter = successful_inversion_counter + successful_inversion_counter_all[i]\n",
    "\n",
    "    # Calculate the correctness\n",
    "    total_inversions = successful_inversion_counter + false_negative_counter\n",
    "    total_entries    = total_inversions + false_positive_counter + no_inversion_counter\n",
    "\n",
    "    # determine the percent correctness\n",
    "    perc_succ            = ( float(successful_inversion_counter + no_inversion_counter) / float(total_entries) ) * 100\n",
    "    perc_succ = round(perc_succ, 2)\n",
    "    perc_succ_inversions = ( float(successful_inversion_counter) / float(total_inversions) ) * 100\n",
    "    perc_succ_inversions = round(perc_succ_inversions, 2)\n",
    "\n",
    "\n",
    "\n",
    "    # Save the data\n",
    "    row = [cutoff, '',\n",
    "          total_entries, successful_inversion_counter, no_inversion_counter, false_positive_counter, false_negative_counter, perc_succ, perc_succ_inversions, '',\n",
    "\n",
    "          total_entries_all[0], successful_inversion_counter_all[0], no_inversion_counter_all[0], false_positive_counter_all[0], false_negative_counter_all[0], perc_succ_all[0], perc_succ_inversions_all[0], '',\n",
    "          total_entries_all[1], successful_inversion_counter_all[1], no_inversion_counter_all[1], false_positive_counter_all[1], false_negative_counter_all[1], perc_succ_all[1], perc_succ_inversions_all[1], '',\n",
    "          total_entries_all[2], successful_inversion_counter_all[2], no_inversion_counter_all[2], false_positive_counter_all[2], false_negative_counter_all[2], perc_succ_all[2], perc_succ_inversions_all[2], '',\n",
    "          total_entries_all[3], successful_inversion_counter_all[3], no_inversion_counter_all[3], false_positive_counter_all[3], false_negative_counter_all[3], perc_succ_all[3], perc_succ_inversions_all[3], '']\n",
    "\n",
    "    with open('./'+filename, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(row)\n",
    "        \n",
    "    cutoff = cutoff - 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
